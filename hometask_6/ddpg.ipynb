{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cc08804d-9595-4b8f-aaa3-f6f3724befd8",
   "metadata": {},
   "source": [
    "# DDPG\n",
    "\n",
    "Задаем структуру аппроксимаций $\\pi^\\eta(s)$, $Q^\\theta(s,a)$ и начальные вектора параметров $\\eta$, $\\theta$.\n",
    "\n",
    "Для каждого эпизода делаем:\n",
    "\n",
    "   Пока эпизод не закончен делаем:\n",
    "\n",
    "- Находясь в состоянии $S_t$ совершаем действие\n",
    "\n",
    "    $$\n",
    "    A_t = \\pi^\\eta(S_t) + Noise,\n",
    "    $$\n",
    "\n",
    "    получаем награду $R_t$  переходим в состояние $S_{t+1}$. Сохраняем \n",
    "    $(S_t,A_t,R_t,D_t,S_{t+1}) \\Rightarrow Memory$\n",
    "\n",
    "\n",
    "- Берем $\\{(s_i,a_i,r_i,d_i,s'_i)\\}_{i=1}^{n} \\leftarrow Memory$, определяем значения\n",
    "\n",
    "    $$\n",
    "    y_i = r_i + (1 - d_i) \\gamma Q^\\theta(s'_i,\\pi^\\eta(s'_i))\n",
    "    $$\n",
    "    функции потерь\n",
    "\n",
    "    $$\n",
    "    Loss_1(\\theta) = \\frac{1}{n}\\sum\\limits_{i=1}^n \\big(y_i - Q^\\theta(s_i,a_i)\\big)^2,\\quad Loss_2(\\eta) = -\\frac{1}{n}\\sum\\limits_{i=1}^n Q^\\theta(s_i,\\pi^\\eta(s_i))\n",
    "    $$\n",
    "\n",
    "    и обновляем вектор параметров\n",
    "\n",
    "    $$\n",
    "    \\theta \\leftarrow \\theta - \\alpha \\nabla_\\theta Loss_1(\\theta),\\quad \\eta \\leftarrow \\eta - \\beta \\nabla_\\eta Loss_2(\\eta),\\quad \\alpha,\\beta > 0\n",
    "    $$\n",
    "\n",
    "- Уменьшаем $Noise$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1cf6f873-d382-4281-83e1-96d7f7845176",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "import random\n",
    "from collections import deque\n",
    "from copy import deepcopy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "805cfc68-8dfd-49f4-9a54-67e3557c07d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ornstein-Uhlenbeck process\n",
    "\n",
    "class OUNoise:\n",
    "    def __init__(self, action_dimension, mu=0, theta=0.15, sigma=0.3):\n",
    "        self.action_dimension = action_dimension\n",
    "        self.mu = mu\n",
    "        self.theta = theta\n",
    "        self.sigma = sigma\n",
    "        self.state = np.ones(self.action_dimension) * self.mu\n",
    "        self.reset()\n",
    "        \n",
    "    def reset(self):\n",
    "        self.state = np.ones(self.action_dimension) * self.mu\n",
    "        \n",
    "    def sample(self):\n",
    "        x = self.state\n",
    "        dx = self.theta * (self.mu - x) + self.sigma * np.random.randn(len(x))\n",
    "        self.state = x + dx\n",
    "        return self.state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "2595212e-89ef-49b9-bd1c-b01aa2cfae68",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self, input_dim, layer1_dim, layer2_dim, output_dim, output_tanh):\n",
    "        super(Net, self).__init__()\n",
    "        self.layer1 = nn.Linear(input_dim, layer1_dim)\n",
    "        self.layer2 = nn.Linear(layer1_dim, layer2_dim)\n",
    "        self.layer3 = nn.Linear(layer2_dim, output_dim)\n",
    "        self.relu = nn.ReLU(True)\n",
    "        self.tanh = nn.Tanh()\n",
    "        self.output_tanh = output_tanh\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.layer1(x))\n",
    "        x = self.relu(self.layer2(x))\n",
    "        x = self.layer3(x)\n",
    "        \n",
    "        if self.output_tanh:\n",
    "            return self.tanh(x)\n",
    "        else:\n",
    "            return x\n",
    "        \n",
    "        \n",
    "class DDPG():\n",
    "    def __init__(self, state_dim, action_dim, action_scale, noise_decrease, \n",
    "                 gamma=0.99, batch_size=64, q_lr=1e-3, pi_lr=1e-4, tau=1e-2, memory_size=100000):\n",
    "        \n",
    "        self.state_dim = state_dim\n",
    "        self.action_dim = action_dim\n",
    "                \n",
    "        self.action_scale = action_scale\n",
    "        \n",
    "        self.pi_model = Net(self.state_dim, 400, 300, self.action_dim, output_tanh=True)\n",
    "        self.q_model = Net(self.state_dim + self.action_dim, 400, 300, 1, output_tanh=False)\n",
    "        self.pi_target_model = deepcopy(self.pi_model)\n",
    "        self.q_target_model = deepcopy(self.q_model)\n",
    "        \n",
    "        \n",
    "        self.noise = OUNoise(self.action_dim)\n",
    "        self.noise_treshold = 1\n",
    "        self.noise_decrease = noise_decrease\n",
    "        \n",
    "        self.tau = tau\n",
    "        self.memory = deque(maxlen=memory_size)\n",
    "        self.batch_size = batch_size\n",
    "        self.gamma = gamma\n",
    "        self.q_optimizer = torch.optim.Adam(self.q_model.parameters(), lr=q_lr)\n",
    "        self.pi_optimizer = torch.optim.Adam(self.q_model.parameters(), lr=pi_lr)\n",
    "        \n",
    "        \n",
    "    def get_action(self, state):\n",
    "        pred_action = self.pi_model(torch.FloatTensor(state)).detach().numpy()\n",
    "        action = self.action_scale * (pred_action + self.noise_treshold * self.noise.sample())\n",
    "        return np.clip(action, -self.action_scale, self.action_scale)\n",
    "    \n",
    "    def update_target_model(self, target_model, model, optimizer, loss):\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        for target_param, param in zip(target_model.parameters(), model.parameters()):\n",
    "            target_param.data.copy_((1 - self.tau) * target_param.data + self.tau * param.data)\n",
    "    \n",
    "    def fit(self, state, action, reward, done, next_state):\n",
    "        self.memory.append([state, action, reward, done, next_state])\n",
    "        \n",
    "        if len(self.memory) > self.batch_size:\n",
    "            batch = random.sample(self.memory, self.batch_size)\n",
    "            states, actions, rewards, dones, next_states = map(torch.FloatTensor, zip(*batch))\n",
    "            rewards = rewards.reshape(self.batch_size, 1)\n",
    "            dones = dones.reshape(self.batch_size, 1)\n",
    "            \n",
    "            pred_next_actions = self.action_scale * self.pi_target_model(next_states)\n",
    "            next_states_and_pred_next_actions = torch.cat((next_states, pred_next_actions), dim=1)\n",
    "            targets = reward + self.gamma * (1 - dones) * self.q_target_model(next_states_and_pred_next_actions)\n",
    "            \n",
    "            states_and_actions = torch.cat((states, actions), dim=1)\n",
    "            temp = (self.q_model(states_and_actions) - targets.detach())\n",
    "            q_loss = torch.mean((targets.detach() - self.q_model(states_and_actions)) ** 2)\n",
    "            self.update_target_model(self.q_target_model, self.q_model, self.q_optimizer, q_loss)\n",
    "            \n",
    "            pred_actions = self.action_scale * self.pi_model(states)\n",
    "            states_and_pred_actions = torch.cat((states, pred_actions), dim=1)\n",
    "            pi_loss = - torch.mean(self.q_model(states_and_pred_actions))\n",
    "            self.update_target_model(self.pi_target_model, self.pi_model, self.pi_optimizer, pi_loss)\n",
    "            \n",
    "            \n",
    "        if self.noise_treshold > 0:\n",
    "            self.noise_trashold = max(0, self.noise_treshold - self.noise_decrease)\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "87595d51-a47f-4416-95a7-94adef6c749e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/verius00/.local/lib/python3.10/site-packages/gym/core.py:317: DeprecationWarning: \u001b[33mWARN: Initializing wrapper in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
      "  deprecation(\n",
      "/home/verius00/.local/lib/python3.10/site-packages/gym/wrappers/step_api_compatibility.py:39: DeprecationWarning: \u001b[33mWARN: Initializing environment in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
      "  deprecation(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode = 0 \t total_reward = -1243.5950399682877\n",
      "episode = 1 \t total_reward = -997.3028878367038\n",
      "episode = 2 \t total_reward = -1236.9298215834053\n",
      "episode = 3 \t total_reward = -776.8356955464615\n",
      "episode = 4 \t total_reward = -1109.8703457349945\n",
      "episode = 5 \t total_reward = -1196.0624739765751\n",
      "episode = 6 \t total_reward = -1184.3271069733041\n",
      "episode = 7 \t total_reward = -1014.8327159695363\n",
      "episode = 8 \t total_reward = -1195.8955020200124\n",
      "episode = 9 \t total_reward = -1747.2333444486503\n",
      "episode = 10 \t total_reward = -1080.580981253478\n",
      "episode = 11 \t total_reward = -1219.1792376183548\n",
      "episode = 12 \t total_reward = -1409.2831809868342\n",
      "episode = 13 \t total_reward = -1169.6283520749369\n",
      "episode = 14 \t total_reward = -1389.2537090027438\n",
      "episode = 15 \t total_reward = -1451.9970700056485\n",
      "episode = 16 \t total_reward = -1267.369915679609\n",
      "episode = 17 \t total_reward = -1444.8755375650105\n",
      "episode = 18 \t total_reward = -1135.8981503020616\n",
      "episode = 19 \t total_reward = -1105.7439530699992\n",
      "episode = 20 \t total_reward = -1091.9935613182893\n",
      "episode = 21 \t total_reward = -1453.6550060109303\n",
      "episode = 22 \t total_reward = -1143.6890858395816\n",
      "episode = 23 \t total_reward = -1312.8184596976657\n",
      "episode = 24 \t total_reward = -1147.9565972756936\n",
      "episode = 25 \t total_reward = -1049.5504327926878\n",
      "episode = 26 \t total_reward = -1047.6373613056232\n",
      "episode = 27 \t total_reward = -1099.042831272719\n",
      "episode = 28 \t total_reward = -1288.9452402697677\n",
      "episode = 29 \t total_reward = -1108.5256291290877\n",
      "episode = 30 \t total_reward = -1225.7145803859487\n",
      "episode = 31 \t total_reward = -1210.8870548954094\n",
      "episode = 32 \t total_reward = -1099.7491590373081\n",
      "episode = 33 \t total_reward = -1283.399881329352\n",
      "episode = 34 \t total_reward = -1393.7660479373349\n",
      "episode = 35 \t total_reward = -1474.5802531567315\n",
      "episode = 36 \t total_reward = -1267.3678536815316\n",
      "episode = 37 \t total_reward = -1224.2137447513092\n",
      "episode = 38 \t total_reward = -1006.4010867038038\n",
      "episode = 39 \t total_reward = -976.5399847633805\n",
      "episode = 40 \t total_reward = -1303.4454003439814\n",
      "episode = 41 \t total_reward = -1025.1517058374227\n",
      "episode = 42 \t total_reward = -982.8244008206032\n",
      "episode = 43 \t total_reward = -1000.813780912908\n",
      "episode = 44 \t total_reward = -1182.4493735322258\n",
      "episode = 45 \t total_reward = -1446.3301163943968\n",
      "episode = 46 \t total_reward = -1226.4671822247176\n",
      "episode = 47 \t total_reward = -1142.2033022711948\n",
      "episode = 48 \t total_reward = -1075.1899383351276\n",
      "episode = 49 \t total_reward = -1202.3043918266853\n",
      "episode = 50 \t total_reward = -1164.2824098954966\n",
      "episode = 51 \t total_reward = -1216.776784662449\n",
      "episode = 52 \t total_reward = -1084.7906617864862\n",
      "episode = 53 \t total_reward = -1160.538978389325\n",
      "episode = 54 \t total_reward = -1224.0349402934492\n",
      "episode = 55 \t total_reward = -1066.578381967284\n",
      "episode = 56 \t total_reward = -1319.4014525937573\n",
      "episode = 57 \t total_reward = -1078.6584343529069\n",
      "episode = 58 \t total_reward = -1507.1345928850178\n",
      "episode = 59 \t total_reward = -1049.886524670853\n",
      "episode = 60 \t total_reward = -1229.8585472680957\n",
      "episode = 61 \t total_reward = -1003.2484048070031\n",
      "episode = 62 \t total_reward = -1078.191945224745\n",
      "episode = 63 \t total_reward = -1029.1830856085908\n",
      "episode = 64 \t total_reward = -1039.174159664543\n",
      "episode = 65 \t total_reward = -1061.4079387307554\n",
      "episode = 66 \t total_reward = -1310.2828634960551\n",
      "episode = 67 \t total_reward = -1012.2785760182876\n",
      "episode = 68 \t total_reward = -1130.0757787819714\n",
      "episode = 69 \t total_reward = -1284.5565132199517\n",
      "episode = 70 \t total_reward = -1067.592415458888\n",
      "episode = 71 \t total_reward = -1027.02956641838\n",
      "episode = 72 \t total_reward = -1419.6645151285948\n",
      "episode = 73 \t total_reward = -1007.8611383466936\n",
      "episode = 74 \t total_reward = -1085.1672687234682\n",
      "episode = 75 \t total_reward = -1409.261707100726\n",
      "episode = 76 \t total_reward = -994.6072431234797\n",
      "episode = 77 \t total_reward = -1176.7972486439103\n",
      "episode = 78 \t total_reward = -1155.269694894912\n",
      "episode = 79 \t total_reward = -1155.5219290840041\n",
      "episode = 80 \t total_reward = -1196.666839343385\n",
      "episode = 81 \t total_reward = -1167.6338905320542\n",
      "episode = 82 \t total_reward = -1227.891493155294\n",
      "episode = 83 \t total_reward = -1070.5123471319564\n",
      "episode = 84 \t total_reward = -1247.0598827389053\n",
      "episode = 85 \t total_reward = -786.7199160650026\n",
      "episode = 86 \t total_reward = -1103.8256961241161\n",
      "episode = 87 \t total_reward = -1172.6499427896465\n",
      "episode = 88 \t total_reward = -1441.0363278443967\n",
      "episode = 89 \t total_reward = -1217.8249636892037\n",
      "episode = 90 \t total_reward = -1089.5508049998673\n",
      "episode = 91 \t total_reward = -1541.4336787355671\n",
      "episode = 92 \t total_reward = -1021.5716311674855\n",
      "episode = 93 \t total_reward = -1188.91415978612\n",
      "episode = 94 \t total_reward = -1016.8121890620384\n",
      "episode = 95 \t total_reward = -1387.9603759202416\n",
      "episode = 96 \t total_reward = -1047.3549774344115\n",
      "episode = 97 \t total_reward = -1138.0122651068104\n",
      "episode = 98 \t total_reward = -1270.1166360698417\n",
      "episode = 99 \t total_reward = -1055.2267400058638\n",
      "episode = 100 \t total_reward = -1181.1397402291998\n",
      "episode = 101 \t total_reward = -1117.184628128662\n",
      "episode = 102 \t total_reward = -1138.6375449488992\n",
      "episode = 103 \t total_reward = -1092.6007847242327\n",
      "episode = 104 \t total_reward = -991.3230966456382\n",
      "episode = 105 \t total_reward = -1198.1208781309763\n",
      "episode = 106 \t total_reward = -1296.5592717185527\n",
      "episode = 107 \t total_reward = -1243.8244340004903\n",
      "episode = 108 \t total_reward = -1436.8900163657577\n",
      "episode = 109 \t total_reward = -1078.6969387740803\n",
      "episode = 110 \t total_reward = -992.3277659504687\n",
      "episode = 111 \t total_reward = -1036.2006105267349\n",
      "episode = 112 \t total_reward = -1123.7784799803608\n",
      "episode = 113 \t total_reward = -1134.8421949175008\n",
      "episode = 114 \t total_reward = -1011.6888074393928\n",
      "episode = 115 \t total_reward = -1490.012072117652\n",
      "episode = 116 \t total_reward = -1098.7940549503198\n",
      "episode = 117 \t total_reward = -1299.1745339452084\n",
      "episode = 118 \t total_reward = -1511.3286760169524\n",
      "episode = 119 \t total_reward = -1294.2907729333729\n",
      "episode = 120 \t total_reward = -1371.6583068105174\n",
      "episode = 121 \t total_reward = -1119.895213219643\n",
      "episode = 122 \t total_reward = -1046.9541530223246\n",
      "episode = 123 \t total_reward = -1231.8678024897254\n",
      "episode = 124 \t total_reward = -1174.7347738128865\n",
      "episode = 125 \t total_reward = -1267.7603802966737\n",
      "episode = 126 \t total_reward = -1195.3372081475927\n",
      "episode = 127 \t total_reward = -1329.443310358643\n",
      "episode = 128 \t total_reward = -1265.385040857937\n",
      "episode = 129 \t total_reward = -953.9667333301517\n",
      "episode = 130 \t total_reward = -1103.5147810643523\n",
      "episode = 131 \t total_reward = -1139.3432179620008\n",
      "episode = 132 \t total_reward = -1102.1694439001121\n",
      "episode = 133 \t total_reward = -1119.8585388875333\n",
      "episode = 134 \t total_reward = -972.5655187977354\n",
      "episode = 135 \t total_reward = -1437.3189170027645\n",
      "episode = 136 \t total_reward = -1124.5308179397384\n",
      "episode = 137 \t total_reward = -1565.68986085997\n",
      "episode = 138 \t total_reward = -1456.4878680156899\n",
      "episode = 139 \t total_reward = -1334.5864277952028\n",
      "episode = 140 \t total_reward = -1430.1859386105466\n",
      "episode = 141 \t total_reward = -1039.3870523528847\n",
      "episode = 142 \t total_reward = -1183.6185364482735\n",
      "episode = 143 \t total_reward = -1228.5358818673988\n",
      "episode = 144 \t total_reward = -993.7978818096773\n",
      "episode = 145 \t total_reward = -1085.934355429726\n",
      "episode = 146 \t total_reward = -1049.7635857183666\n",
      "episode = 147 \t total_reward = -1084.2375368250428\n",
      "episode = 148 \t total_reward = -1018.374822662636\n",
      "episode = 149 \t total_reward = -1230.3925498727133\n",
      "episode = 150 \t total_reward = -1092.1791714859864\n",
      "episode = 151 \t total_reward = -1310.4611417715562\n",
      "episode = 152 \t total_reward = -1389.62483984387\n",
      "episode = 153 \t total_reward = -1323.0480781730053\n",
      "episode = 154 \t total_reward = -1080.4979933128634\n",
      "episode = 155 \t total_reward = -1127.7453841846095\n",
      "episode = 156 \t total_reward = -951.8422166969245\n",
      "episode = 157 \t total_reward = -1207.433055836997\n",
      "episode = 158 \t total_reward = -1135.701800038282\n",
      "episode = 159 \t total_reward = -1166.0617166566133\n",
      "episode = 160 \t total_reward = -1009.7048546697798\n",
      "episode = 161 \t total_reward = -1146.4436323248424\n",
      "episode = 162 \t total_reward = -1167.9913123117162\n",
      "episode = 163 \t total_reward = -974.1663914065223\n",
      "episode = 164 \t total_reward = -1040.8927944115424\n",
      "episode = 165 \t total_reward = -1256.9889238332387\n",
      "episode = 166 \t total_reward = -1175.1287908815902\n",
      "episode = 167 \t total_reward = -990.4444008528977\n",
      "episode = 168 \t total_reward = -976.336837628049\n",
      "episode = 169 \t total_reward = -934.4357841223344\n",
      "episode = 170 \t total_reward = -1182.3632495832694\n",
      "episode = 171 \t total_reward = -1179.6529013398988\n",
      "episode = 172 \t total_reward = -1161.9640275650686\n",
      "episode = 173 \t total_reward = -1137.7485065974092\n",
      "episode = 174 \t total_reward = -1049.4621243614904\n",
      "episode = 175 \t total_reward = -1117.3897596409515\n",
      "episode = 176 \t total_reward = -1110.2757392279918\n",
      "episode = 177 \t total_reward = -1214.4457649636336\n",
      "episode = 178 \t total_reward = -1468.9431144559753\n",
      "episode = 179 \t total_reward = -1010.7456362920532\n",
      "episode = 180 \t total_reward = -1263.681521851019\n",
      "episode = 181 \t total_reward = -1155.8795756045156\n",
      "episode = 182 \t total_reward = -961.8554919466548\n",
      "episode = 183 \t total_reward = -1015.5299176305604\n",
      "episode = 184 \t total_reward = -1081.5059803831796\n",
      "episode = 185 \t total_reward = -1057.936140569297\n",
      "episode = 186 \t total_reward = -1017.4104501698771\n",
      "episode = 187 \t total_reward = -1063.4011854765865\n",
      "episode = 188 \t total_reward = -1066.3419663238342\n",
      "episode = 189 \t total_reward = -1230.6407620057096\n",
      "episode = 190 \t total_reward = -1230.3571620400692\n",
      "episode = 191 \t total_reward = -1455.701996436265\n",
      "episode = 192 \t total_reward = -1050.0824106146185\n",
      "episode = 193 \t total_reward = -1007.2861919472185\n",
      "episode = 194 \t total_reward = -1188.6535543626767\n",
      "episode = 195 \t total_reward = -1088.3688962369479\n",
      "episode = 196 \t total_reward = -1075.5475550508986\n",
      "episode = 197 \t total_reward = -1173.651524026008\n",
      "episode = 198 \t total_reward = -1246.46155839421\n",
      "episode = 199 \t total_reward = -1291.2112058557643\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "\n",
    "env = gym.make(\"Pendulum-v1\")\n",
    "\n",
    "state_dim = 3\n",
    "action_dim = 1\n",
    "action_scale = 2\n",
    "\n",
    "\n",
    "episode_n = 200\n",
    "trajectory_len = 200\n",
    "\n",
    "noise_decrease = 1 / (episode_n * trajectory_len)\n",
    "\n",
    "agent = DDPG(state_dim, action_dim, action_scale, noise_decrease)\n",
    "\n",
    "\n",
    "for episode in range(episode_n):\n",
    "    \n",
    "    total_reward = 0\n",
    "    state = env.reset()\n",
    "    for _ in range(trajectory_len):\n",
    "        \n",
    "        action = agent.get_action(state)\n",
    "        next_state, reward, done, _ = env.step(action)\n",
    "        \n",
    "        agent.fit(state, action, reward, done, next_state)\n",
    "        total_reward += reward\n",
    "        if done:\n",
    "            break\n",
    "            \n",
    "        state = next_state\n",
    "    print(f'{episode = } \\t {total_reward = }')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
