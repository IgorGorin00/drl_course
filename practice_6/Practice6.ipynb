{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a5fa9052-b5f4-4f93-94cc-a414a6a4ac7a",
   "metadata": {},
   "source": [
    "# DDPG\n",
    "\n",
    "Задаем структуру аппроксимаций $\\pi^\\eta(s)$, $Q^\\theta(s,a)$ и начальные вектора параметров $\\eta$, $\\theta$.\n",
    "\n",
    "Для каждого эпизода делаем:\n",
    "\n",
    "   Пока эпизод не закончен делаем:\n",
    "\n",
    "- Находясь в состоянии $S_t$ совершаем действие\n",
    "\n",
    "    $$\n",
    "    A_t = \\pi^\\eta(S_t) + Noise,\n",
    "    $$\n",
    "\n",
    "    получаем награду $R_t$  переходим в состояние $S_{t+1}$. Сохраняем \n",
    "    $(S_t,A_t,R_t,D_t,S_{t+1}) \\Rightarrow Memory$\n",
    "\n",
    "\n",
    "- Берем $\\{(s_i,a_i,r_i,d_i,s'_i)\\}_{i=1}^{n} \\leftarrow Memory$, определяем значения\n",
    "\n",
    "    $$\n",
    "    y_i = r_i + (1 - d_i) \\gamma Q^\\theta(s'_i,\\pi^\\eta(s'_i))\n",
    "    $$\n",
    "    функции потерь\n",
    "\n",
    "    $$\n",
    "    Loss_1(\\theta) = \\frac{1}{n}\\sum\\limits_{i=1}^n \\big(y_i - Q^\\theta(s_i,a_i)\\big)^2,\\quad Loss_2(\\eta) = -\\frac{1}{n}\\sum\\limits_{i=1}^n Q^\\theta(s_i,\\pi^\\eta(s_i))\n",
    "    $$\n",
    "\n",
    "    и обновляем вектор параметров\n",
    "\n",
    "    $$\n",
    "    \\theta \\leftarrow \\theta - \\alpha \\nabla_\\theta Loss_1(\\theta),\\quad \\eta \\leftarrow \\eta - \\beta \\nabla_\\eta Loss_2(\\eta),\\quad \\alpha,\\beta > 0\n",
    "    $$\n",
    "\n",
    "- Уменьшаем $Noise$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6bb4fdb2-5023-42f3-a7ba-0def3c69aa56",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Ornstein–Uhlenbeck process (Процесс Орнштейна – Уленбека)\n",
    "\n",
    "class OUNoise:\n",
    "    def __init__(self, action_dimension, mu=0, theta=0.15, sigma=0.3):\n",
    "        self.action_dimension = action_dimension\n",
    "        self.mu = mu\n",
    "        self.theta = theta\n",
    "        self.sigma = sigma\n",
    "        self.state = np.ones(self.action_dimension) * self.mu\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.state = np.ones(self.action_dimension) * self.mu\n",
    "\n",
    "    def sample(self):\n",
    "        x = self.state\n",
    "        dx = self.theta * (self.mu - x) + self.sigma * np.random.randn(len(x))\n",
    "        self.state = x + dx\n",
    "        return self.state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "641708d2-a56b-4645-a5f3-b7b368a4ea75",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import random\n",
    "from collections import deque\n",
    "from copy import deepcopy\n",
    "\n",
    "\n",
    "class TwoLayersNeuralNetwork(nn.Module):\n",
    "    def __init__(self, input_dim, layer1_dim, layer2_dim, output_dim, output_tanh):\n",
    "        super().__init__()\n",
    "        self.layer1 = nn.Linear(input_dim, layer1_dim)\n",
    "        self.layer2 = nn.Linear(layer1_dim, layer2_dim)\n",
    "        self.layer3 = nn.Linear(layer2_dim, output_dim)\n",
    "        self.output_tanh = output_tanh\n",
    "        self.relu = nn.ReLU()\n",
    "        self.tanh = nn.Tanh()\n",
    "        \n",
    "    def forward(self, input):\n",
    "        hidden = self.layer1(input)\n",
    "        hidden = self.relu(hidden)\n",
    "        hidden = self.layer2(hidden)\n",
    "        hidden = self.relu(hidden)\n",
    "        output = self.layer3(hidden)\n",
    "        \n",
    "        if self.output_tanh:\n",
    "            return self.tanh(output)\n",
    "        else:\n",
    "            return output\n",
    "        \n",
    "        \n",
    "class DDPG():\n",
    "    def __init__(self, state_dim, action_dim, action_scale, noise_decrease,\n",
    "                 gamma=0.99, batch_size=64, q_lr=1e-3, pi_lr=1e-4, tau=1e-2, memory_size=100000):\n",
    "        self.state_dim = state_dim\n",
    "        self.action_dim = action_dim\n",
    "        self.action_scale = action_scale\n",
    "        self.pi_model = TwoLayersNeuralNetwork(self.state_dim, 400, 300, self.action_dim, output_tanh=True)\n",
    "        self.q_model = TwoLayersNeuralNetwork(self.state_dim + self.action_dim, 400, 300, 1, output_tanh=False)\n",
    "        self.pi_target_model = deepcopy(self.pi_model)\n",
    "        self.q_target_model = deepcopy(self.q_model)\n",
    "        self.noise = OUNoise(self.action_dim)\n",
    "        self.noise_threshold = 1\n",
    "        self.noise_decrease = noise_decrease\n",
    "        self.gamma = gamma\n",
    "        self.tau = tau\n",
    "        self.batch_size = batch_size\n",
    "        self.q_optimazer = torch.optim.Adam(self.q_model.parameters(), lr=q_lr)\n",
    "        self.pi_optimazer = torch.optim.Adam(self.pi_model.parameters(), lr=pi_lr)\n",
    "        self.memory = deque(maxlen=memory_size)\n",
    "    \n",
    "    def get_action(self, state):\n",
    "        pred_action = self.pi_model(torch.FloatTensor(state)).detach().numpy()\n",
    "        action = self.action_scale * (pred_action + self.noise_threshold * self.noise.sample())\n",
    "        return np.clip(action, -self.action_scale, self.action_scale)\n",
    "    \n",
    "    def update_target_model(self, target_model, model, optimazer, loss):\n",
    "        optimazer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimazer.step()\n",
    "        for target_param, param in zip(target_model.parameters(), model.parameters()):\n",
    "            target_param.data.copy_((1 - self.tau) * target_param.data + self.tau * param.data) \n",
    "    \n",
    "    \n",
    "    def fit(self, state, action, reward, done, next_state):\n",
    "        self.memory.append([state, action, reward, done, next_state])\n",
    "        \n",
    "        if len(self.memory) > self.batch_size:\n",
    "            batch = random.sample(self.memory, self.batch_size)\n",
    "            states, actions, rewards, dones, next_states = map(torch.FloatTensor, zip(*batch))\n",
    "            rewards = rewards.reshape(self.batch_size, 1)\n",
    "            dones = dones.reshape(self.batch_size, 1)\n",
    "            \n",
    "            pred_next_actions = self.action_scale * self.pi_target_model(next_states)\n",
    "            next_states_and_pred_next_actions = torch.cat((next_states, pred_next_actions), dim=1)\n",
    "            targets = rewards + self.gamma * (1 - dones) * self.q_target_model(next_states_and_pred_next_actions)\n",
    "            \n",
    "            states_and_actions = torch.cat((states, actions), dim=1)\n",
    "            temp = (self.q_model(states_and_actions) - targets.detach())\n",
    "            q_loss = torch.mean((targets.detach() - self.q_model(states_and_actions)) ** 2)\n",
    "            self.update_target_model(self.q_target_model, self.q_model, self.q_optimazer, q_loss)\n",
    "            \n",
    "            pred_actions = self.action_scale * self.pi_model(states)\n",
    "            states_and_pred_actions = torch.cat((states, pred_actions), dim=1)\n",
    "            pi_loss = - torch.mean(self.q_model(states_and_pred_actions))\n",
    "            self.update_target_model(self.pi_target_model, self.pi_model, self.pi_optimazer, pi_loss)\n",
    "            \n",
    "        if self.noise_threshold > 0:\n",
    "            self.noise_threshold = max(0, self.noise_threshold - self.noise_decrease)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "99e90bfa-fa82-4c7a-9174-bcbbbead4fd4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode=0, total_reward=-1108.616163818342\n",
      "episode=1, total_reward=-1596.197309476567\n",
      "episode=2, total_reward=-1583.6923841612388\n",
      "episode=3, total_reward=-1395.2619158806754\n",
      "episode=4, total_reward=-1489.9827300427573\n",
      "episode=5, total_reward=-1397.1149769273266\n",
      "episode=6, total_reward=-1091.0409703218447\n",
      "episode=7, total_reward=-900.9874957314875\n",
      "episode=8, total_reward=-1386.3344270545208\n",
      "episode=9, total_reward=-1108.796744891272\n",
      "episode=10, total_reward=-884.177551468509\n",
      "episode=11, total_reward=-999.1052511324599\n",
      "episode=12, total_reward=-1003.7644847073448\n",
      "episode=13, total_reward=-884.7378875609694\n",
      "episode=14, total_reward=-897.004524533428\n",
      "episode=15, total_reward=-766.7312655336788\n",
      "episode=16, total_reward=-752.0811845887091\n",
      "episode=17, total_reward=-882.8837374820451\n",
      "episode=18, total_reward=-775.4640726110194\n",
      "episode=19, total_reward=-737.0042409923736\n",
      "episode=20, total_reward=-509.2619020790743\n",
      "episode=21, total_reward=-390.28583026022125\n",
      "episode=22, total_reward=-510.85400942622846\n",
      "episode=23, total_reward=-501.2920542182292\n",
      "episode=24, total_reward=-370.8499492836013\n",
      "episode=25, total_reward=-508.51889432018663\n",
      "episode=26, total_reward=-326.9285202420582\n",
      "episode=27, total_reward=-628.0038081550872\n",
      "episode=28, total_reward=-490.42714724430266\n",
      "episode=29, total_reward=-247.29961971336567\n",
      "episode=30, total_reward=-127.2737186728553\n",
      "episode=31, total_reward=-243.06173170404568\n",
      "episode=32, total_reward=-369.2965362890392\n",
      "episode=33, total_reward=-375.52671624521366\n",
      "episode=34, total_reward=-381.0842401602218\n",
      "episode=35, total_reward=-482.22603838640623\n",
      "episode=36, total_reward=-253.0303029587396\n",
      "episode=37, total_reward=-365.5047653815471\n",
      "episode=38, total_reward=-242.13140507124808\n",
      "episode=39, total_reward=-1.9297117839614708\n",
      "episode=40, total_reward=-125.65548878922861\n",
      "episode=41, total_reward=-373.3795526662134\n",
      "episode=42, total_reward=-127.01063616832505\n",
      "episode=43, total_reward=-124.05644788831218\n",
      "episode=44, total_reward=-492.3165205619765\n",
      "episode=45, total_reward=-5.476715078432797\n",
      "episode=46, total_reward=-119.79145078952911\n",
      "episode=47, total_reward=-252.12204168396744\n",
      "episode=48, total_reward=-119.67766767506818\n",
      "episode=49, total_reward=-2.0436117064598793\n",
      "episode=50, total_reward=-118.37616545431693\n",
      "episode=51, total_reward=-0.8466188065097079\n",
      "episode=52, total_reward=-124.58508597881288\n",
      "episode=53, total_reward=-124.80543330365994\n",
      "episode=54, total_reward=-249.0882892021836\n",
      "episode=55, total_reward=-253.7263709813184\n",
      "episode=56, total_reward=-120.6541452578131\n",
      "episode=57, total_reward=-249.07506674505058\n",
      "episode=58, total_reward=-252.83327462082391\n",
      "episode=59, total_reward=-408.19204198151886\n",
      "episode=60, total_reward=-348.72003885475215\n",
      "episode=61, total_reward=-126.54940698971394\n",
      "episode=62, total_reward=-300.5196547855294\n",
      "episode=63, total_reward=-253.86543010700913\n",
      "episode=64, total_reward=-119.24213968778774\n",
      "episode=65, total_reward=-128.17105573657537\n",
      "episode=66, total_reward=-126.11736553666185\n",
      "episode=67, total_reward=-119.00082395117248\n",
      "episode=68, total_reward=-126.27396450328453\n",
      "episode=69, total_reward=-128.70416426194666\n",
      "episode=70, total_reward=-247.65774659146862\n",
      "episode=71, total_reward=-120.18237414554677\n",
      "episode=72, total_reward=-121.44386853254993\n",
      "episode=73, total_reward=-244.9428611407233\n",
      "episode=74, total_reward=-246.2552026121337\n",
      "episode=75, total_reward=-126.01551448008452\n",
      "episode=76, total_reward=-124.15083065987633\n",
      "episode=77, total_reward=-123.04194062176934\n",
      "episode=78, total_reward=-121.49182075770631\n",
      "episode=79, total_reward=-233.59087660308023\n",
      "episode=80, total_reward=-117.1092919560032\n",
      "episode=81, total_reward=-238.79386189998013\n",
      "episode=82, total_reward=-327.7666699508436\n",
      "episode=83, total_reward=-322.05024515276693\n",
      "episode=84, total_reward=-126.41903997861363\n",
      "episode=85, total_reward=-3.4274681585976037\n",
      "episode=86, total_reward=-131.0727276205815\n",
      "episode=87, total_reward=-364.79537950016874\n",
      "episode=88, total_reward=-123.97186204331757\n",
      "episode=89, total_reward=-344.85241127659094\n",
      "episode=90, total_reward=-236.97298072088415\n",
      "episode=91, total_reward=-243.50848594214867\n",
      "episode=92, total_reward=-255.56928308891298\n",
      "episode=93, total_reward=-121.79642410034766\n",
      "episode=94, total_reward=-126.51048826044656\n",
      "episode=95, total_reward=-120.81772222596814\n",
      "episode=96, total_reward=-228.9217868823973\n",
      "episode=97, total_reward=-118.30138254610947\n",
      "episode=98, total_reward=-127.06339762531131\n",
      "episode=99, total_reward=-314.93063391538857\n",
      "episode=100, total_reward=-122.0845914829852\n",
      "episode=101, total_reward=-125.0235801105822\n",
      "episode=102, total_reward=-343.72167169901985\n",
      "episode=103, total_reward=-125.43568827346019\n",
      "episode=104, total_reward=-343.4191879600737\n",
      "episode=105, total_reward=-124.382361231206\n",
      "episode=106, total_reward=-120.89355017409916\n",
      "episode=107, total_reward=-241.0267087785698\n",
      "episode=108, total_reward=-127.38439061518403\n",
      "episode=109, total_reward=-246.23836388922513\n",
      "episode=110, total_reward=-123.36002324741959\n",
      "episode=111, total_reward=-118.19103523087487\n",
      "episode=112, total_reward=-121.00455145899808\n",
      "episode=113, total_reward=-124.94943214360704\n",
      "episode=114, total_reward=-124.11514306388175\n",
      "episode=115, total_reward=-4.523488727270816\n",
      "episode=116, total_reward=-127.93623122487529\n",
      "episode=117, total_reward=-123.79376485911726\n",
      "episode=118, total_reward=-235.6626363623023\n",
      "episode=119, total_reward=-227.39356227630296\n",
      "episode=120, total_reward=-124.74877012278792\n",
      "episode=121, total_reward=-245.10871738033552\n",
      "episode=122, total_reward=-125.26820921375236\n",
      "episode=123, total_reward=-232.5656216123104\n",
      "episode=124, total_reward=-126.78153954077142\n",
      "episode=125, total_reward=-121.54932112491083\n",
      "episode=126, total_reward=-232.04188961304092\n",
      "episode=127, total_reward=-122.06972894124125\n",
      "episode=128, total_reward=-129.79237945909213\n",
      "episode=129, total_reward=-228.45180053045092\n",
      "episode=130, total_reward=-247.0665113050365\n",
      "episode=131, total_reward=-128.85265195087692\n",
      "episode=132, total_reward=-121.634751155347\n",
      "episode=133, total_reward=-128.10177510801935\n",
      "episode=134, total_reward=-228.9595108697365\n",
      "episode=135, total_reward=-121.27042835587014\n",
      "episode=136, total_reward=-128.4216810541151\n",
      "episode=137, total_reward=-3.4298678312675457\n",
      "episode=138, total_reward=-240.29889726599313\n",
      "episode=139, total_reward=-123.0209474017802\n",
      "episode=140, total_reward=-128.08030194834745\n",
      "episode=141, total_reward=-246.84518485636656\n",
      "episode=142, total_reward=-3.449930778166062\n",
      "episode=143, total_reward=-342.97009456614137\n",
      "episode=144, total_reward=-122.61413629033615\n",
      "episode=145, total_reward=-125.15680887788402\n",
      "episode=146, total_reward=-120.52684171537925\n",
      "episode=147, total_reward=-117.75361520568173\n",
      "episode=148, total_reward=-124.98831819213159\n",
      "episode=149, total_reward=-2.978454634938715\n",
      "episode=150, total_reward=-125.7987789552079\n",
      "episode=151, total_reward=-127.99290428124291\n",
      "episode=152, total_reward=-123.29357749267521\n",
      "episode=153, total_reward=-124.40470406285584\n",
      "episode=154, total_reward=-123.94771002979965\n",
      "episode=155, total_reward=-126.98871645677899\n",
      "episode=156, total_reward=-235.71854161332178\n",
      "episode=157, total_reward=-3.204354050221433\n",
      "episode=158, total_reward=-131.20284922070266\n",
      "episode=159, total_reward=-120.85220184104499\n",
      "episode=160, total_reward=-123.39973161755528\n",
      "episode=161, total_reward=-250.2127463486977\n",
      "episode=162, total_reward=-234.65468859858018\n",
      "episode=163, total_reward=-122.36530342551222\n",
      "episode=164, total_reward=-300.12908927941504\n",
      "episode=165, total_reward=-244.13247017050077\n",
      "episode=166, total_reward=-119.62879453472101\n",
      "episode=167, total_reward=-117.01022463138895\n",
      "episode=168, total_reward=-2.133531027960211\n",
      "episode=169, total_reward=-124.88581877469349\n",
      "episode=170, total_reward=-122.53314448035697\n",
      "episode=171, total_reward=-122.42470125691801\n",
      "episode=172, total_reward=-242.15225765559924\n",
      "episode=173, total_reward=-122.32557734711452\n",
      "episode=174, total_reward=-122.70491498329383\n",
      "episode=175, total_reward=-119.70924713244446\n",
      "episode=176, total_reward=-239.08438697598595\n",
      "episode=177, total_reward=-124.3871707920982\n",
      "episode=178, total_reward=-126.77213378583633\n",
      "episode=179, total_reward=-2.514119044375827\n",
      "episode=180, total_reward=-125.68511354395584\n",
      "episode=181, total_reward=-122.58923292703297\n",
      "episode=182, total_reward=-121.87696628270255\n",
      "episode=183, total_reward=-119.8036448564681\n",
      "episode=184, total_reward=-343.92681801540607\n",
      "episode=185, total_reward=-240.44197664391382\n",
      "episode=186, total_reward=-241.88434341287189\n",
      "episode=187, total_reward=-128.05341178311946\n",
      "episode=188, total_reward=-124.16153295179745\n",
      "episode=189, total_reward=-122.227291069427\n",
      "episode=190, total_reward=-123.87326541899742\n",
      "episode=191, total_reward=-117.55027048540029\n",
      "episode=192, total_reward=-337.4604010958896\n",
      "episode=193, total_reward=-244.9096531829705\n",
      "episode=194, total_reward=-2.390443167564767\n",
      "episode=195, total_reward=-118.70264171144214\n",
      "episode=196, total_reward=-358.0903821297143\n",
      "episode=197, total_reward=-118.13492799870207\n",
      "episode=198, total_reward=-127.75375488932416\n",
      "episode=199, total_reward=-316.2619482744018\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "\n",
    "episode_n = 200\n",
    "trajectory_len = 200\n",
    "\n",
    "env = gym.make('Pendulum-v1')\n",
    "agent = DDPG(state_dim=3, action_dim=1, action_scale=2, noise_decrease = 1 / (episode_n * trajectory_len))\n",
    "\n",
    "for episode in range(episode_n):\n",
    "    \n",
    "    total_reward = 0\n",
    "    state = env.reset()\n",
    "    for _ in range(trajectory_len):\n",
    "        action = agent.get_action(state)\n",
    "        next_action, reward, done, _ = env.step(action)\n",
    "        total_reward += reward\n",
    "        \n",
    "        agent.fit(state, action, reward, done, next_action)\n",
    "        \n",
    "        if done:\n",
    "            break\n",
    "            \n",
    "        state = next_action\n",
    "    \n",
    "    print(f'episode={episode}, total_reward={total_reward}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91e8d8d1-f028-4df9-ae2f-28825827fcd8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
