{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4d29d125-ea09-4e86-911d-cf8de43c19fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import gym\n",
    "\n",
    "import time\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f93a760c-e314-4638-a8e2-520b555982a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CEMAgent(nn.Module):\n",
    "    def __init__(self, state_n, action_n, lr, opt_f=torch.optim.Adam):\n",
    "        super(CEMAgent, self).__init__()\n",
    "        self.state_n = state_n\n",
    "        self.action_n = action_n\n",
    "        self.lr = lr\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(self.state_n, 5),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(5, self.action_n))\n",
    "        \n",
    "        self.softmax = nn.Softmax()\n",
    "        self.loss_f = nn.CrossEntropyLoss()\n",
    "        self.opt = opt_f(self.parameters(), lr=self.lr)\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "    \n",
    "    def get_action(self, state):\n",
    "        state = torch.FloatTensor(state)\n",
    "        logits = self(state)\n",
    "        probs = self.softmax(logits)\n",
    "        action = np.random.choice(self.action_n, p=probs.detach().numpy())\n",
    "        return action\n",
    "    def update_policy(self, elite_trajectories):\n",
    "        elite_states, elite_actions = [], []\n",
    "        for trajectory in elite_trajectories:\n",
    "            elite_states.extend(trajectory['states'])\n",
    "            elite_actions.extend(trajectory['actions'])\n",
    "            \n",
    "        elite_states = torch.FloatTensor(elite_states)\n",
    "        elite_actions = torch.LongTensor(elite_actions)\n",
    "        loss = self.loss_f(self(elite_states), elite_actions)\n",
    "        loss.backward()\n",
    "        self.opt.step()\n",
    "        self.opt.zero_grad()\n",
    "        return loss.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "43cc355c-3954-4512-80d4-851bcaec2218",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_trajectory(trajectory_len, env, agent):\n",
    "    trajectory = {'states': [], 'actions': [], 'reward': 0}\n",
    "    state = env.reset()\n",
    "    trajectory['states'] += [state]\n",
    "    for _ in range(trajectory_len):\n",
    "        action = agent.get_action(state)\n",
    "        state, reward, done, _ = env.step(action)\n",
    "        trajectory['actions'] += [action]\n",
    "        trajectory['reward'] += reward\n",
    "        if done:\n",
    "            break\n",
    "        trajectory['states'] += [state]\n",
    "    return trajectory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "88a2edde-2554-4636-87e5-d1621961e020",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_elite_trajectories(trajectories, q_param):\n",
    "    rewards = [trajectory['reward'] for trajectory in trajectories]\n",
    "    q_value = np.quantile(rewards, q_param)\n",
    "    return np.mean(rewards), [trajectory for trajectory in trajectories if trajectory['reward'] > q_value]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0fa4f29a-57bb-494e-82bc-97388ce99cf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(epochs, env, agent, traj_per_epoch, traj_len, q_param):\n",
    "    start = time.perf_counter()\n",
    "    history = []\n",
    "    for epoch in range(epochs):\n",
    "        loss = 0\n",
    "        trajectories = [get_trajectory(traj_len, env, agent) for _ in range(traj_per_epoch)]\n",
    "        mean_reward, elite_trajectories = get_elite_trajectories(trajectories, q_param)\n",
    "        history += [mean_reward]\n",
    "        if len(elite_trajectories) > 0: \n",
    "            loss = agent.update_policy(elite_trajectories)\n",
    "        print(f'{epoch=}, {loss=}, {mean_reward=}')\n",
    "    end = time.perf_counter()\n",
    "    print(f'Training took {round(end-start, 5)}')\n",
    "    return history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a3e72e2e-f291-447b-b4dd-7b2fce089f5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/verius00/.local/lib/python3.10/site-packages/gym/core.py:317: DeprecationWarning: \u001b[33mWARN: Initializing wrapper in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
      "  deprecation(\n",
      "/home/verius00/.local/lib/python3.10/site-packages/gym/wrappers/step_api_compatibility.py:39: DeprecationWarning: \u001b[33mWARN: Initializing environment in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
      "  deprecation(\n",
      "/tmp/ipykernel_5727/123197339.py:21: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  probs = self.softmax(logits)\n",
      "/tmp/ipykernel_5727/123197339.py:30: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at  ../torch/csrc/utils/tensor_new.cpp:204.)\n",
      "  elite_states = torch.FloatTensor(elite_states)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch=0, loss=0.6883884072303772, mean_reward=22.4\n",
      "epoch=1, loss=0.6734998822212219, mean_reward=27.77\n",
      "epoch=2, loss=0.656034529209137, mean_reward=35.23\n",
      "epoch=3, loss=0.6410014629364014, mean_reward=39.16\n",
      "epoch=4, loss=0.6343991756439209, mean_reward=40.24\n",
      "epoch=5, loss=0.6196318864822388, mean_reward=48.61\n",
      "epoch=6, loss=0.6039091944694519, mean_reward=49.56\n",
      "epoch=7, loss=0.5854854583740234, mean_reward=55.22\n",
      "epoch=8, loss=0.5794401168823242, mean_reward=56.82\n",
      "epoch=9, loss=0.5746863484382629, mean_reward=61.17\n",
      "epoch=10, loss=0.5696635842323303, mean_reward=68.91\n",
      "epoch=11, loss=0.5547099113464355, mean_reward=82.39\n",
      "epoch=12, loss=0.5460835695266724, mean_reward=78.4\n",
      "epoch=13, loss=0.5378401875495911, mean_reward=96.82\n",
      "epoch=14, loss=0.5218889117240906, mean_reward=116.98\n",
      "epoch=15, loss=0.5323563814163208, mean_reward=178.06\n",
      "epoch=16, loss=0.5283113121986389, mean_reward=243.79\n",
      "epoch=17, loss=0.5257488489151001, mean_reward=270.16\n",
      "epoch=18, loss=0, mean_reward=320.13\n",
      "epoch=19, loss=0, mean_reward=314.73\n",
      "epoch=20, loss=0, mean_reward=325.44\n",
      "epoch=21, loss=0, mean_reward=340.09\n",
      "epoch=22, loss=0.5137203931808472, mean_reward=305.33\n",
      "epoch=23, loss=0, mean_reward=376.22\n",
      "epoch=24, loss=0, mean_reward=359.61\n",
      "epoch=25, loss=0, mean_reward=356.72\n",
      "epoch=26, loss=0, mean_reward=360.04\n",
      "epoch=27, loss=0, mean_reward=341.98\n",
      "epoch=28, loss=0, mean_reward=383.05\n",
      "epoch=29, loss=0, mean_reward=355.37\n",
      "epoch=30, loss=0, mean_reward=348.28\n",
      "epoch=31, loss=0, mean_reward=357.13\n",
      "epoch=32, loss=0, mean_reward=376.78\n",
      "epoch=33, loss=0, mean_reward=355.77\n",
      "epoch=34, loss=0, mean_reward=341.81\n",
      "epoch=35, loss=0, mean_reward=351.4\n",
      "epoch=36, loss=0, mean_reward=360.88\n",
      "epoch=37, loss=0, mean_reward=340.39\n",
      "epoch=38, loss=0, mean_reward=370.47\n",
      "epoch=39, loss=0, mean_reward=364.48\n",
      "epoch=40, loss=0, mean_reward=358.4\n",
      "epoch=41, loss=0, mean_reward=356.51\n",
      "epoch=42, loss=0, mean_reward=371.5\n",
      "epoch=43, loss=0, mean_reward=381.9\n",
      "epoch=44, loss=0, mean_reward=377.45\n",
      "epoch=45, loss=0, mean_reward=341.74\n",
      "epoch=46, loss=0, mean_reward=367.52\n",
      "epoch=47, loss=0, mean_reward=370.3\n",
      "epoch=48, loss=0, mean_reward=379.22\n",
      "epoch=49, loss=0, mean_reward=351.96\n",
      "epoch=50, loss=0, mean_reward=342.3\n",
      "Training took 260.00585\n"
     ]
    }
   ],
   "source": [
    "env = gym.make('CartPole-v1')\n",
    "\n",
    "state_n, action_n = 4, 2\n",
    "lr = 0.1\n",
    "agent = CEMAgent(state_n=state_n, action_n=action_n, lr=lr)\n",
    "\n",
    "epochs = 51\n",
    "traj_per_epoch = 100\n",
    "traj_len = 500\n",
    "q_param = 0.8\n",
    "\n",
    "history = train(epochs, env, agent, traj_per_epoch, traj_len, q_param)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
