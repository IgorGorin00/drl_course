# -*- coding: utf-8 -*-
"""dqn_gym_acrobot_hard_soft_double.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1EdMo05patMh4ZnPCbytcISXQONifTSWY
"""

# Commented out IPython magic to ensure Python compatibility.
import gym

import torch
import torch.nn as nn

import copy

import time
from tqdm.notebook import tqdm
import numpy as np
from random import sample
import matplotlib.pyplot as plt
# %matplotlib inline

"""# Deep Q-learning"""

device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
device

class NN(nn.Module):
    def __init__(self, state_dim, action_n):
        super(NN, self).__init__()

        self.linear1 = nn.Linear(state_dim, 32)
        self.linear2 = nn.Linear(32, 32)
        self.linear3 = nn.Linear(32, action_n)
        self.relu = nn.ReLU(inplace=True)
            
    def forward(self, x):
        x = torch.FloatTensor(np.array(x))
        x = self.relu(self.linear1(x))
        x = self.relu(self.linear2(x))
        x = self.linear3(x)
        return x


class DQN():
    def __init__(self, action_n, batch_size, trajectory_n):
        
        self.action_n = action_n
        self.epsilon = 1
        self.epsilon_decrease = 1 / trajectory_n
        self.memory = []
        self.batch_size = batch_size

    def get_action(self, state, model):

        qvalues = model(state).detach().numpy()
        probs = np.ones(self.action_n) * self.epsilon / self.action_n
        agrmax_action = np.argmax(qvalues)
        probs[agrmax_action] += 1 - self.epsilon
        action = np.random.choice(self.action_n, p=probs)

        return action

    def get_batch(self):
        batch = sample(population=self.memory, k=self.batch_size)

        states, actions, rewards, dones, next_states = [], [], [], [], []

        for i in range(self.batch_size):
            states.append(batch[i][0])
            actions.append(batch[i][1])
            rewards.append(batch[i][2])
            dones.append(batch[i][3])
            next_states.append(batch[i][4])

        return states, actions, rewards, dones, next_states


    def training_step(self, state, action, reward, done, next_state, gamma, target_model, training_model, double=False):
        
        self.memory.append([state, action, reward, done, next_state])
        
        if len(self.memory) > self.batch_size * 10:
            
            states, actions, rewards, dones, next_states = self.get_batch()



            q = training_model(states)
            
            if not double:
                q_next = training_model(next_states)
            else:
                q_next = target_model(next_states)

            targets = q.clone()

            for i in range(self.batch_size):
                targets[i][actions[i]] = rewards[i] + (1 - dones[i]) * gamma * torch.max(q_next[i])

            loss = torch.mean((targets.detach() - q) ** 2)
            self.epsilon = max(0, self.epsilon - self.epsilon_decrease)
            
            return loss

"""# Training loop"""

def train_dqn(epochs, traj_per_epoch, method,
                          agent, target_model, training_model, 
                          env, trajectory_len, action_n, 
                          batch_size, gamma, tau, lr, opt_f=torch.optim.SGD):


    history = {'train_rewards': [], 'train_losses': [], 'val_rewards': [], 'checkpoints': []}

    opt = opt_f(training_model.parameters(), lr=lr)
    
    for epoch in range(epochs):

        
        training_model.load_state_dict(target_model.state_dict())
        
        epoch_loss = 0
        epoch_reward = 0
        training_model.train()        
        for traj_i in tqdm(range(traj_per_epoch)):
            
            state = env.reset()
            
            trajectory_loss = 0
            trajectory_reward = 0
            
            for _ in range(trajectory_len):
                
                action = agent.get_action(state, training_model)
                
                next_state, reward, done, info = env.step(action)
                
                if method != 'double':
                    loss = agent.training_step(state, action, reward, done, next_state, gamma, target_model, training_model, double=False)
                else:
                    loss = agent.training_step(state, action, reward, done, next_state, gamma, target_model, training_model, double=True)
                
                if loss is not None:
                    loss.backward()
                    opt.step()
                    opt.zero_grad()
                    trajectory_loss += loss
                
                trajectory_reward += reward
                state = next_state
                if done: 
                    break
            epoch_loss += trajectory_loss
            epoch_reward += trajectory_reward
        
        checkpoint = training_model.state_dict()    
        real_model = target_model.state_dict()

        if method == 'hard':
            target_model.load_state_dict(checkpoint)

        elif method == 'soft' or method == 'double':
            for p_f, p_r in zip(checkpoint, real_model):
                real_model[p_r] = (checkpoint[p_f] * tau) + (real_model[p_r] * (1 - tau))


        target_model.load_state_dict(real_model)
        
        epoch_loss /= traj_per_epoch
        epoch_reward /= traj_per_epoch
        history['train_losses'].append(epoch_loss)
        history['train_rewards'].append(epoch_reward)
        history['checkpoints'].append(checkpoint)
        
        state = env.reset()
        val_reward = 0
        target_model.eval()
        for _ in range(trajectory_len):
            action = agent.get_action(state, target_model)
            state, reward, done, info = env.step(action)
            val_reward += reward
            if done: 
                break
        history['val_rewards'].append(val_reward)
        print(f'{epoch = } \t mean_{epoch_loss = } \t mean_{epoch_reward = } \t {val_reward = }')
    
    return history

"""# Comparison"""

env = gym.make("Acrobot-v1")


state_dim = env.observation_space.shape[0]
action_n = env.action_space.n

epochs = 15
traj_per_epoch = 150
batch_size = 64

target_model = NN(state_dim=state_dim, action_n=action_n)
training_model = NN(state_dim=state_dim, action_n=action_n)
agent = DQN(action_n=action_n, batch_size=batch_size, trajectory_n=traj_per_epoch)

trajectory_len = 500

gamma = 0.99
lr = 1e-2
opt_f = torch.optim.Adam


tau = 0.8
method = 'soft'
history_soft_dqn = train_dqn(epochs, traj_per_epoch, method,
                               agent, target_model, training_model, 
                               env, trajectory_len, action_n, 
                               batch_size, gamma, tau, lr, opt_f=torch.optim.SGD)

epochs = 40
traj_per_epoch = 50
batch_size = 64

target_model = NN(state_dim=state_dim, action_n=action_n)
training_model = NN(state_dim=state_dim, action_n=action_n)
agent = DQN(action_n=action_n, batch_size=batch_size, trajectory_n=traj_per_epoch)

trajectory_len = 500

gamma = 0.99
lr = 1e-2
opt_f = torch.optim.Adam


tau = 0.8
method = 'double'
history_double_dqn = train_dqn(epochs, traj_per_epoch, method,
                               agent, target_model, training_model, 
                               env, trajectory_len, action_n, 
                               batch_size, gamma, tau, lr, opt_f=torch.optim.SGD)

with plt.style.context('ggplot'):
    plt.figure(figsize=(20, 9))
    plt.plot(history_soft_dqn['train_rewards'], label='Epoch mean reward (train)')
    plt.plot(history_soft_dqn['val_rewards'], label='Epoch val rewards (real model)')
    plt.title('DQN soft update rewards')
    plt.legend()

with plt.style.context('ggplot'):
    plt.figure(figsize=(20, 9))
    plt.plot(history_double_dqn['train_rewards'], label='Epoch mean reward (train)')
    plt.plot(history_double_dqn['val_rewards'], label='Epoch val rewards (real model)')
    plt.title('Double DQN rewards')
    plt.legend()


'''
def get_trajectory(env, agent, t_len):
    state = env.reset()
    tot_reward = 0
    for _ in range(t_len):
        action = agent.get_action(state)
        state, reward, done, _ = env.step(action)
        tot_reward += reward
        if done:
            break
    return tot_reward

for checkpoint in history_soft_update['checkpoints']:
    target_agent.model.load_state_dict(checkpoint)
    print(get_trajectory(env, training_agent, trajectory_len))

'''