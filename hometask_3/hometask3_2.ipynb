{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "87d74bb3-fbc5-4be7-94c2-bafb5dde2bca",
   "metadata": {},
   "outputs": [],
   "source": [
    "from Frozen_Lake import FrozenLakeEnv\n",
    "import numpy as np\n",
    "import time\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "06eccf7c-daa3-407c-9d1e-7b7e72f030b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = FrozenLakeEnv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a5eec3f2-d51d-4668-9a7e-21c36e288845",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_states = env.get_all_states()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "66aea527-0b1d-4067-bf9e-4afbaf221991",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('left', 'down', 'right', 'up')\n",
      "('left', 'down', 'right', 'up')\n",
      "('left', 'down', 'right', 'up')\n",
      "('left', 'down', 'right', 'up')\n",
      "('left', 'down', 'right', 'up')\n",
      "()\n",
      "('left', 'down', 'right', 'up')\n",
      "()\n",
      "('left', 'down', 'right', 'up')\n",
      "('left', 'down', 'right', 'up')\n",
      "('left', 'down', 'right', 'up')\n",
      "()\n",
      "()\n",
      "('left', 'down', 'right', 'up')\n",
      "('left', 'down', 'right', 'up')\n",
      "()\n"
     ]
    }
   ],
   "source": [
    "for state in all_states:\n",
    "    print(env.get_possible_actions(state))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "32a2fa9f-4c43-4c88-96c2-19f9a1f996a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*FFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "\n"
     ]
    }
   ],
   "source": [
    "env.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e27a0d45-84d3-4caf-a8ca-59ff2d3d8d5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_policy(env):\n",
    "    policy = {}\n",
    "    for state in env.get_all_states():\n",
    "        policy[state] = {}\n",
    "        for action in env.get_possible_actions(state):\n",
    "            policy[state][action] = 1 / len(env.get_possible_actions(state))\n",
    "    return policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e9804c4e-7329-4204-83be-fc41648cd21c",
   "metadata": {},
   "outputs": [],
   "source": [
    "policy = init_policy(env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "79d5360d-cc7e-4271-bc66-dabab386998c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def policy_evaluation_step(policy, values, gamma):\n",
    "    q_values = get_q_values(values, gamma)\n",
    "    new_values = {}\n",
    "    for state in env.get_all_states():\n",
    "        new_values[state] = 0\n",
    "        for action in env.get_possible_actions(state):\n",
    "            new_values[state] += policy[state][action] * q_values[state][action]\n",
    "    return new_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a5c36946-e1e9-4d1b-98f6-0d3606833e3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_values():\n",
    "    return {state: 0 for state in env.get_all_states()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "04ea7279-9b4a-4203-88e4-a5baabefb5c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "values = init_values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e4ec7a6e-8f71-444a-a1df-dcdccf2eec7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_q_values(values, gamma):\n",
    "    q_values = {}\n",
    "    for state in env.get_all_states():\n",
    "        q_values[state] = {}\n",
    "        for action in env.get_possible_actions(state):\n",
    "            q_values[state][action] = 0\n",
    "            for next_state in env.get_next_states(state, action):\n",
    "                reward = env.get_reward(state, action, next_state)\n",
    "                transition_prob = env.get_transition_prob(state, action, next_state)\n",
    "                next_value = values[next_state]\n",
    "                q_values[state][action] += reward + gamma * transition_prob  * next_value\n",
    "                \n",
    "    return q_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cdc20a60-5ce0-4965-95a7-d801c731a8e3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{(0, 0): 0.0,\n",
       " (0, 1): 0.0,\n",
       " (0, 2): 0.0,\n",
       " (0, 3): 0.0,\n",
       " (1, 0): 0.0,\n",
       " (1, 1): 0,\n",
       " (1, 2): 0.0,\n",
       " (1, 3): 0,\n",
       " (2, 0): 0.0,\n",
       " (2, 1): 0.0,\n",
       " (2, 2): 0.0,\n",
       " (2, 3): 0,\n",
       " (3, 0): 0,\n",
       " (3, 1): 0.0,\n",
       " (3, 2): 0.75,\n",
       " (3, 3): 0}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "values = policy_evaluation_step(policy, values, gamma=0.9)\n",
    "values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "96f8e071-4721-46a0-9a0d-0b7267d60658",
   "metadata": {},
   "outputs": [],
   "source": [
    "def policy_evaluation(policy, values, gamma, evaluation_step_n):\n",
    "    \n",
    "    for _ in range(evaluation_step_n):\n",
    "        values = policy_evaluation_step(policy, values, gamma)\n",
    "    q_values = get_q_values(values, gamma)\n",
    "    return q_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "69af887f-c86b-49f6-b430-5039eb86aa5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def policy_evaluation_with_resetting(policy, gamma, evaluation_step_n):\n",
    "    values = init_values()\n",
    "    for _ in range(evaluation_step_n):\n",
    "        values = policy_evaluation_step(policy, values, gamma)\n",
    "    q_values = get_q_values(values, gamma)\n",
    "    return q_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "aa7f25a1-ff47-497e-8671-cf1cd863627d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{(0, 0): {'left': 0.012694672242094042,\n",
       "  'down': 0.01686835383364476,\n",
       "  'right': 0.01214429542378969,\n",
       "  'up': 0.012019806754971888},\n",
       " (0, 1): {'left': 0.010810946369245896,\n",
       "  'down': 0.003926884642953772,\n",
       "  'right': 0.02288425734126018,\n",
       "  'up': 0.013047390910457366},\n",
       " (0, 2): {'left': 0.0189486317796474,\n",
       "  'down': 0.059132792337086376,\n",
       "  'right': 0.01872347762670558,\n",
       "  'up': 0.023996176355580403},\n",
       " (0, 3): {'left': 0.022856113072142453,\n",
       "  'down': 0.003829943271548001,\n",
       "  'right': 0.010007271128881995,\n",
       "  'up': 0.012725295386109774},\n",
       " (1, 0): {'left': 0.020770851485299637,\n",
       "  'down': 0.042155416251302716,\n",
       "  'right': 0.00625142132081882,\n",
       "  'up': 0.01148581185636805},\n",
       " (1, 1): {},\n",
       " (1, 2): {'left': 0.031600450021849295,\n",
       "  'down': 0.23105940611697212,\n",
       "  'right': 0.031600450021849295,\n",
       "  'up': 0.02174419405782223},\n",
       " (1, 3): {},\n",
       " (2, 0): {'left': 0.042155416251302716,\n",
       "  'down': 0.0205964531631862,\n",
       "  'right': 0.1262460665953071,\n",
       "  'up': 0.03511588332766702},\n",
       " (2, 1): {'left': 0.07554391068366573,\n",
       "  'down': 0.31555237232309924,\n",
       "  'right': 0.26626282931989526,\n",
       "  'up': 0.03392498669971434},\n",
       " (2, 2): {'left': 0.23724358232830495,\n",
       "  'down': 0.8611726382172289,\n",
       "  'right': 0.11281244450355797,\n",
       "  'up': 0.07243470226742157},\n",
       " (2, 3): {},\n",
       " (3, 0): {},\n",
       " (3, 1): {'left': 0.050757315431016484,\n",
       "  'down': 0.3873297288720269,\n",
       "  'right': 0.896376061420152,\n",
       "  'up': 0.23013348107338893},\n",
       " (3, 2): {'left': 0.4162121546366484,\n",
       "  'down': 1.8808221691920588,\n",
       "  'right': 1.1345847690132633,\n",
       "  'up': 1.2662628293198952},\n",
       " (3, 3): {}}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q_values = policy_evaluation(policy, values, gamma=0.9, evaluation_step_n=100)\n",
    "q_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7febf107-0f48-4a81-b6fb-214224c97518",
   "metadata": {},
   "outputs": [],
   "source": [
    "def policy_improvement(q_values):\n",
    "    new_policy = {}\n",
    "    for state in env.get_all_states():\n",
    "        new_policy[state] = {}\n",
    "        max_action = None\n",
    "        max_q_value = float('-inf')\n",
    "        for action in env.get_possible_actions(state):\n",
    "            if q_values[state][action] > max_q_value:\n",
    "                max_q_value = q_values[state][action]\n",
    "                max_action = action\n",
    "        for action in env.get_possible_actions(state):\n",
    "            new_policy[state][action] = 1 if action == max_action else 0\n",
    "    return new_policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3d7d5331-0ec6-46a0-9c83-abdf3363db22",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{(0, 0): {'left': 0, 'down': 1, 'right': 0, 'up': 0},\n",
       " (0, 1): {'left': 0, 'down': 0, 'right': 1, 'up': 0},\n",
       " (0, 2): {'left': 0, 'down': 1, 'right': 0, 'up': 0},\n",
       " (0, 3): {'left': 1, 'down': 0, 'right': 0, 'up': 0},\n",
       " (1, 0): {'left': 0, 'down': 1, 'right': 0, 'up': 0},\n",
       " (1, 1): {},\n",
       " (1, 2): {'left': 0, 'down': 1, 'right': 0, 'up': 0},\n",
       " (1, 3): {},\n",
       " (2, 0): {'left': 0, 'down': 0, 'right': 1, 'up': 0},\n",
       " (2, 1): {'left': 0, 'down': 1, 'right': 0, 'up': 0},\n",
       " (2, 2): {'left': 0, 'down': 1, 'right': 0, 'up': 0},\n",
       " (2, 3): {},\n",
       " (3, 0): {},\n",
       " (3, 1): {'left': 0, 'down': 0, 'right': 1, 'up': 0},\n",
       " (3, 2): {'left': 0, 'down': 1, 'right': 0, 'up': 0},\n",
       " (3, 3): {}}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "policy_improvement(q_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "877f6046-bb5d-4c32-8268-56aa699704cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(epochs, evaluation_step_n, gamma, env, reset_values=False):\n",
    "    policy = init_policy(env)\n",
    "    for epoch in range(epochs):\n",
    "        if reset_values:\n",
    "            q_values = policy_evaluation_with_resetting(policy, gamma, evaluation_step_n)\n",
    "        else: \n",
    "            q_values = policy_evaluation(policy, values, gamma, evaluation_step_n)\n",
    "        policy = policy_improvement(q_values)\n",
    "\n",
    "    return policy\n",
    "\n",
    "\n",
    "\n",
    "def test(policy, vizualize=False):\n",
    "    total_reward = 0\n",
    "    state = env.reset()\n",
    "    for _ in range(100):\n",
    "        action = np.random.choice(env.get_possible_actions(state), p=list(policy[state].values()))\n",
    "        state, reward, done, _ = env.step(action)\n",
    "        if vizualize:\n",
    "            env.render()\n",
    "        total_reward += reward\n",
    "\n",
    "        if done:\n",
    "            break\n",
    "\n",
    "    return total_reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "b1f1dcd7-8dca-4440-ab63-7504854e96f2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5e93b986087242f08e914c83ab834015",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d0b6750365c14e7784ec6fec28db842f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "epochs = 20\n",
    "evaluation_step_n = 100\n",
    "gamma = 0.9\n",
    "\n",
    "policy_with_resetting = train(epochs, evaluation_step_n, gamma, env, reset_values=False)\n",
    "policy_without_resetting = train(epochs, evaluation_step_n, gamma, env, reset_values=False)\n",
    "\n",
    "\n",
    "rewards_with_resetting = [test(policy_with_resetting) for _ in tqdm(range(10000))]\n",
    "rewards_without_resetting = [test(policy_without_resetting) for _ in tqdm(range(10000))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "efebebd2-f989-49cd-a883-32e42fd9b263",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean reward for 10_000 tries with resetting [0.7373]\n",
      "Mean reward for 10_000 tries with resetting [0.7452]\n"
     ]
    }
   ],
   "source": [
    "print(f'Mean reward for 10_000 tries with resetting [{np.mean(rewards_with_resetting)}]')\n",
    "print(f'Mean reward for 10_000 tries with resetting [{np.mean(rewards_without_resetting)}]')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
